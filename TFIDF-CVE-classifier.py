#/bin/python

"""
Created on March 1 15:32:03 2022

@Summary: Command line tool that categorized vulnerabilities based on LDA (TF-IDF) ML model

@author: Robert Roslonek

@Requires-Python: >=3

@Maintainer-email: "Robert Roslonek" 

@License: GPL version 3

"""

import os,re
import os.path
import pandas as pd
import numpy as np  
import json as json
from nltk.tokenize import RegexpTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer


 #TF-IDF (term frequency-inverse document frequency) is a feature engineering technique where the most important words are extracted by taking into account their frequency in documents and across the entire list of documents as a whole.

# Topic modeling is an unsupervised learning method where groups of words that often appear together are clustered into topics. Typically, the words in one topic should be related and make sense (e.g in our case Java CVEs or OpenSSL topics). Individual documents (CVEs) can fall under one topic or multiple topics.

#In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical (topic modeling) model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.

#According to LDA (LatentDirichletAllocation) every word is associated (or related) with a latent (or hidden) topic.
# in proposed algorithm LDA is used to analyse list of found CVEs and words from each CVE Synopsis field (out of Nessus format)  to train model with possible distinct number of categories -  20 as default and deriving from there a way to map huge number of CVEs into small number of classes.

# In LDA, documents (here each CVEs Synopsis field) are considered to be bags of words and words are considered to be independent given the topics (i.e., word order is irrelevant). When LDA is run, the various distributions are learned using Bayesian inference. In effect, the generative model just described is run backwards and the distributions are updated from their priors using the actual documents

#For more details about LTA TF-IDF categorisation technique look here https://machinelearninggeek.com/latent-dirichlet-allocation-using-scikit-learn/


# ================== Configuration

trainDataF='your_trainData_from_NESSUS_scan results.csv'

testDataF='your_testData _from_Nessus.csv'

#Mapping file to map LDA topic (by word match) with user provided classes
mapping_file="map_of_mitigations_updated.json"

#nessus csv file encoding, for properly reading content
encoding_scheme='windows-1252'

#######---------------------- Define the number of topics/CVE classes  
#10 better than 25 better than 50
num_topics=20


#one or 2 words (n-grams)
topic_size=(1,1)

########--------------------



#Preprocessing
#Tokenize the CVEs
#Remove stop words
#Perform stemming on text article
def preprocess_data(doc_set):
    """
    Input  : docuemnt list
    Purpose: preprocess text (tokenize, removing stopwords, and stemming)
    Output : preprocessed text
    """
    # initialize regex tokenizer
    tokenizer = RegexpTokenizer(r'\w+')
    # create English stop words list
    en_stop = set(stopwords.words('english'))
    # Create p_stemmer of class PorterStemmer
    p_stemmer = PorterStemmer()
    # list for tokenized documents in loop
    texts = []
    # loop through document list
    for i in doc_set:
        # clean and tokenize document string
        raw = i.lower()
        tokens = tokenizer.tokenize(raw)
        # remove stop words from tokens
        stopped_tokens = [i for i in tokens if not i in en_stop]
        # stem tokens
        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
        # add tokens to list
        texts.append(stemmed_tokens)
    return texts

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append(" ".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))
    return texts_out







# Load Dataset
input = pd.read_csv(trainDataF, error_bad_lines=False);
testData = pd.read_csv(trainDataF, error_bad_lines=False);

#get workds and remove NAN
#get only CVEs out of test/train data
trainData=input.loc[input['CVE'] != ""]


test_documents_list= testData['Synopsis'].fillna(' ').tolist()
documents_list= trainData['Synopsis'].fillna(' ').tolist()

print("Size of documents: "+ str(len(documents_list)))



# Label every LDA found topic with regex pattern of Remediation
#topic2terms is an array terms arrays
#expected format topics2terms[index]=[*top_terms_list]
def topic2cat_mapping(topic2terms, mapping_f):
   topics2categories=dict()
   currentDir=os.path.dirname(os.path.abspath(__file__))
   file_exists = os.path.exists(currentDir+"\\"+mapping_f)
   if (not file_exists  ): 
    print ("File not found in current directory"+mapping_f)
    print ("Skipping mapping LDA categories to custom Regexes")
    return
	
   
   #enforce file encoding format due to some specifics in readnig csv file
   with open(mapping_f, newline='', encoding=encoding_scheme) as csvfile:
   

    patterns = json.load(csvfile)
   
    print ("\nListing recognized categories:\n")
    for topic in topic2terms:

      for pattern in  patterns["Mapping"]:

       source=", ".join(topic2terms[topic])

       if re.search(pattern["re1"], source):
        print ("Topic "+str(topic)+" => " + pattern['category'].upper()+" srctext:"+source )
        print (20*"-") 
        topics2categories[topic]=pattern['category']
        break
    return topics2categories    
    
#read mapping
topics_categories=[]

#Common stop words from online
stops= [
"a", "about", "above", "across", "after", "afterwards", 
"again", "all", "almost", "alone", "along", "already", "also",    
"although", "always", "am", "among", "amongst", "amoungst", "amount", "an", "and", "another", "any", "anyhow", "anyone", "anything", "anyway", "anywhere", "are", "as", "at", "be", "became", "because", "become","becomes", "becoming", "been", "before", "behind", "being", "beside", "besides", "between", "beyond", "both", "but", "by","can", "cannot", "cant", "could", "couldnt", "de", "describe", "do", "done", "each", "eg", "either", "else", "enough", "etc", "even", "ever", "every", "everyone", "everything", "everywhere", "except", "few", "find","for","found", "four", "from", "further", "get", "give", "go", "had", "has", "hasnt", "have", "he", "hence", "her", "here", "hereafter", "hereby", "herein", "hereupon", "hers", "herself", "him", "himself", "his", "how", "however", "i", "ie", "if", "in", "indeed", "is", "it", "its", "itself", "keep", "least", "less", "ltd", "made", "many", "may", "me", "meanwhile", "might", "mine", "more", "moreover", "most", "mostly", "much", "must", "my", "myself", "name", "namely", "neither", "never", "nevertheless", "next","no", "nobody", "none", "noone", "nor", "not", "nothing", "now", "nowhere", "of", "off", "often", "on", "once", "one", "only", "onto", "or", "other", "others", "otherwise", "our", "ours", "ourselves", "out", "over", "own", "part","perhaps", "please", "put", "rather", "re", "same", "see", "seem", "seemed", "seeming", "seems", "she", "should","since", "sincere","so", "some", "somehow", "someone", "something", "sometime", "sometimes", "somewhere", "still", "such", "take","than", "that", "the", "their", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "therefore", "therein", "thereupon", "these", "they","this", "those", "though", "through", "throughout","thru", "thus", "to", "together", "too", "toward", "towards","under", "until", "up", "upon", "us","very", "was", "we", "well", "were", "what", "whatever", "when","whence", "whenever", "where", "whereafter", "whereas", "whereby","wherein", "whereupon", "wherever", "whether", "which", "while", "who", "whoever", "whom", "whose", "why", "will", "with","within", "without", "would", "yet", "you", "your", "yours", "yourself", "yourselves"]

# Initialize regex tokenizer
tokenizer = RegexpTokenizer(r'\w+')


#Some fine tunning of or TFIDF model
#ngram_range param which will be used for deciding if the vocabulary will contain uniqrams, or bigrams or trigrams etc:-
#min_df=20 Filter those terms that appear in < 20 documents.
#max_df=1000 Filter those terms that appear in > 1000 documents.
#lowercase=False Do not convert text to lowercase. Default is True.
#ngram_range=2 Include phrases of length 2, instead of just single words

# Vectorize document using TF-IDF , stop-words from english template
tfidf = TfidfVectorizer(lowercase=True,
                        min_df=20,
                        max_df=200,
                        stop_words='english',
                        ngram_range = topic_size,
                        tokenizer = tokenizer.tokenize)


# Fit and Transform the documents
train_data = tfidf.fit_transform(documents_list)  

#get all identified words
# Create LDA object
model=LatentDirichletAllocation(n_components=num_topics,max_iter=20, learning_method='online', random_state = 42)

# Fit and Transform SVD model on data
lda_matrix = model.fit_transform(train_data)


#A model with higher log-likelihood and lower perplexity (exp(-1. * log-likelihood per word)) is considered to be good. Letâ€™s check for our model.

# Log Likelyhood: Higher the better
print("Log Likelihood: ", model.score(train_data))

# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)
print("Perplexity: ", model.perplexity(train_data))

# See model parameters
print(model.get_params())
print("\n")

# perplexity might not be the best measure to evaluate topic models because it doesnâ€™t consider the context and semantic associations between words.
#This can be captured using topic coherence measure, an example of this is described in the gensim tutorial I mentioned earlier.

# Get Components  = components (number of topics)
lda_components=model.components_


sorting = np.argsort(model.components_, axis = 1)[:, ::-1]
feature_names = np.array(tfidf.get_feature_names_out())


#sort our documents
docs = np.argsort(documents_list)


## all elements, reversed
do=docs[::-1]


# Print the topics with their terms
terms = tfidf.get_feature_names_out()

topics2terms=dict()
#topics
for index, component in enumerate(lda_components):
    zipped = zip(terms, component)
    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]
    top_terms_list=list(dict(top_terms_key).keys())
    print("Topic "+str(index)+": ",top_terms_list)
    #array of topics with every topic containng array of words/terms
    topics2terms[index]=[*top_terms_list]
	#print ("Topic "+str(topic)+" => " + pattern['category'].upper()+" srctext:"+source )
    
 
mapping=topic2cat_mapping(topics2terms,mapping_file)

doc_topic_distrib=model.transform(train_data)

#THIS PRINTS DOC -> TOPIC mapping
doc_topic=lda_components
print ("Processed matrix")
print (doc_topic_distrib.shape)

